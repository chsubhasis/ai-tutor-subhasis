{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4LKx4lG_NZk",
        "outputId": "807d1414-2fa5-4acd-842e-0e3a3a052ff3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-11-22 14:12:34--  https://raw.githubusercontent.com/AlaFalaki/tutorial_notebooks/main/data/mini-llama-articles.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 173646 (170K) [text/plain]\n",
            "Saving to: ‘mini-llama-articles.csv.1’\n",
            "\n",
            "\r          mini-llam   0%[                    ]       0  --.-KB/s               \rmini-llama-articles 100%[===================>] 169.58K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2024-11-22 14:12:34 (37.7 MB/s) - ‘mini-llama-articles.csv.1’ saved [173646/173646]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/AlaFalaki/tutorial_notebooks/main/data/mini-llama-articles.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBX22xX6_nUQ"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain-core\n",
        "!pip -q install langchain-community\n",
        "!pip -q install langchain_huggingface\n",
        "!pip -q install langchain_chroma\n",
        "!pip -q install PyPDF2\n",
        "!pip -q install transformers\n",
        "!pip -q install datasets\n",
        "!pip -q install -U accelerate bitsandbytes peft trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaGBbgN8biYV",
        "outputId": "ce1bfbce-65b1-4b8e-a71f-15a61ff218c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter you HuggingFace access token:··········\n"
          ]
        }
      ],
      "source": [
        "# Authentication for Huggingface API\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "hfapi_key = getpass(\"Enter you HuggingFace access token:\")\n",
        "os.environ[\"HF_TOKEN\"] = hfapi_key\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hfapi_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3g9pUd51TK6",
        "outputId": "c6697cb1-225d-418e-d936-06683bb6d565"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "rows = []\n",
        "\n",
        "# Load the file as a JSON\n",
        "with open(\"mini-llama-articles.csv\", mode=\"r\", encoding=\"utf-8\") as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "\n",
        "    for idx, row in enumerate(csv_reader):\n",
        "        if idx == 0:\n",
        "            continue\n",
        "            # Skip header row\n",
        "        rows.append(row)\n",
        "\n",
        "# The number of characters in the dataset.\n",
        "len(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0J9Ig86W1vCi"
      },
      "outputs": [],
      "source": [
        "#load rows into documents\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "#documents = [Document(page_content=row[1]) for row in rows]\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=row[1], metadata={\"title\": row[0], \"url\": row[2], \"source_name\": row[3]}\n",
        "    )\n",
        "    for row in rows\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRbBI-qNzdH8",
        "outputId": "6230511d-7279-4bea-d30a-3b634feb2d8b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sicDYB00EdD"
      },
      "outputs": [],
      "source": [
        "#print first entry from documents\n",
        "documents[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "As8JS_IIzq-d",
        "outputId": "2b1ff4ff-8552-4075-91d1-ad7e677b36e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'title': \"Beyond GPT-4: What's New?\",\n",
              " 'url': 'https://pub.towardsai.net/beyond-gpt-4-whats-new-cbd61a448eb9#dda8',\n",
              " 'source_name': 'towards_ai'}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cv9z3nZYAZII"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 512,\n",
        "    chunk_overlap = 128\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "U1cE-PzyAbPX",
        "outputId": "b8d81ffa-1f5f-4237-e27f-7970287f5f02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "448\n",
            "499\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"LLM Variants and Meta's Open Source Before shedding light on four major trends, I'd share the latest Meta's Llama 2 and Code Llama. Meta's Llama 2 represents a sophisticated evolution in LLMs. This suite spans models pretrained and fine-tuned across a parameter spectrum of 7 billion to 70 billion. A specialized derivative, Llama 2-Chat, has been engineered explicitly for dialogue-centric applications. Benchmarking revealed Llama 2's superior performance over most extant open-source chat models.\""
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "splits = text_splitter.split_documents(documents)\n",
        "\n",
        "print(len(splits))\n",
        "print(len(splits[0].page_content) )\n",
        "splits[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "etnH4g1aAjPY",
        "outputId": "5be754fc-a8c1-4be9-d7c2-91c8719f69ee"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"The rise of Vector DBs represents a broader trend in AI: the quest for more efficient, scalable, and versatile data handling solutions. As we navigate this evolution, it's clear that the combination of LLMs and Vector DBs will redefine how we store, access, and understand data in the AI-driven future.  From Agents to OS The AI realm is abuzz with innovations, and one of the most intriguing shifts we're witnessing is the transition from LLM agents to using LLMs as Operating Systems (OS). Let's delve into\""
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "splits[10].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUPMPbwRAlwX",
        "outputId": "084d9c0b-f61e-4f5f-cd04-342b0d8a204d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YjuEUcGrA04Y",
        "outputId": "2554b7a2-6a0a-47f4-b800-8db99a990453"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "modelPath=\"mixedbread-ai/mxbai-embed-large-v1\"\n",
        "\n",
        "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
        "model_kwargs = {'device': device}      # cuda/cpu\n",
        "\n",
        "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "\n",
        "embedding =  HuggingFaceEmbeddings(\n",
        "    model_name=modelPath,     # Provide the pre-trained model's path\n",
        "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
        "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzwdoiPEC0Sc",
        "outputId": "65d95842-a07d-42fd-ac72-823db3818869"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "HuggingFaceEmbeddings(model_name='mixedbread-ai/mxbai-embed-large-v1', cache_folder=None, model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': False}, multi_process=False, show_progress=False)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lljxCsfoCNck"
      },
      "outputs": [],
      "source": [
        "# importing HuggingFace model abstraction class from langchain\n",
        "from langchain_huggingface import HuggingFaceEndpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwi5A9idCO_l",
        "outputId": "065b3c95-d994-478c-f48a-209216b71c11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    #repo_id=\"chsubhasis/ai-tutor-towardsai\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens = 512,\n",
        "    top_k = 10,\n",
        "    temperature = 0.1,\n",
        "    repetition_penalty = 1.03,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJdWhdYFfMld",
        "outputId": "89dc8e26-5e7b-4a5f-9ad6-cc7686bb238c"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFaceEndpoint(repo_id='chsubhasis/ai-tutor-towardsai', huggingfacehub_api_token='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', top_k=10, temperature=0.1, repetition_penalty=1.03, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='chsubhasis/ai-tutor-towardsai', client=<InferenceClient(model='chsubhasis/ai-tutor-towardsai', timeout=120)>, async_client=<InferenceClient(model='chsubhasis/ai-tutor-towardsai', timeout=120)>, task='text-generation')"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.cache import InMemoryCache\n",
        "from langchain.globals import set_llm_cache\n",
        "set_llm_cache(InMemoryCache())"
      ],
      "metadata": {
        "id": "mPt7YKmPvN_7"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToT6unyVCoBd",
        "outputId": "bd36b935-39b6-4533-94d2-4bc33100d43a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Tim Cook is the CEO of Apple, having taken over from Steve Jobs in 2011. Cook joined Apple in 1998 and has held various executive positions within the company, including senior vice president of worldwide operations and chief operating officer. Under Cook's leadership, Apple has continued to innovate and grow, with the company's market capitalization reaching over $2 trillion in 2021. Cook is known for his focus on privacy, sustainability, and social responsibility, and has been recognized as one of the most influential business leaders in the world.\n"
          ]
        }
      ],
      "source": [
        "# Test a sample response\n",
        "response = llm.invoke(\"Who is the CEO of Apple?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "tzURSzJ8C4MM"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "persist_directory = 'docs/chroma/'\n",
        "!rm -rf ./docs/chroma  # remove old database files if any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "VHrWmldmDVac"
      },
      "outputs": [],
      "source": [
        "vectordb = Chroma.from_documents(\n",
        "    documents=splits, # splits we created earlier\n",
        "    embedding=embedding,\n",
        "    persist_directory=persist_directory, # save the directory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-Q8qI2TnMKEk"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "vectordb = Chroma(\n",
        "    persist_directory=persist_directory,\n",
        "    embedding_function=embedding\n",
        ")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZmiYB6DSxb0",
        "outputId": "7791794f-99ef-4d9b-b425-77ddadc78b83"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_chroma.vectorstores.Chroma at 0x7f36cd7c5a50>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bELMwLmdDoM7",
        "outputId": "833a8180-b5ad-42a1-873d-754364d8d5c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "448\n"
          ]
        }
      ],
      "source": [
        "print(vectordb._collection.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "Gr5fPj0WDuF7"
      },
      "outputs": [],
      "source": [
        "question = \"What is Artificial Intelligence?\"\n",
        "#docs = vectordb.similarity_search(question, k=5)     # k --> No. of doc to return\n",
        "docs = vectordb.search(question, search_type=\"mmr\", k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baDF0gM2DxO7",
        "outputId": "a10765b8-c43c-4937-8b5d-80d65f5e1a8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iterations on massive amounts of text data, the network learns the patterns in the texts and the nuances of the language - enough to generate coherent words, sentences, or whole documents by itself. The artificial neural network is the main feature of a subset of machine learning called deep learning. It is very important in the field of AI due to its ability to capture intricate patterns and dependencies in data and generalize from these patterns to make predictions on new, unseen data. In the context of\n",
            "AI Engineering AI engineering is a burgeoning discipline combining software engineering principles with AI techniques to design, build, and scale intelligent systems. As large-scale foundation models continue to revolutionize the AI landscape, AI engineering plays a pivotal role in their development and deployment. AI engineering offers the tools and techniques necessary to scale out large-scale models while maintaining their performance and adaptability. Some aspects of scaling out these models through\n",
            "What is Generative AI? Generative AI is a subfield of machine learning that involves training artificial intelligence models on large volumes of real-world data to generate new contents (text, image, code,...) that is comparable to what humans would create. This is achieved by training algorithms on large datasets to identify patterns and learn from them. Once the neural network has learned these patterns, it can generate new data that adheres to the same patterns. However, this process is computationally\n",
            "as Facebook Artificial Intelligence Research (FAIR), is an artificial intelligence laboratory that aims to share open-source frameworks, tools, libraries, and models for research exploration and large-scale production deployment. In 2018, they released the open-source PyText, a modeling framework focused on NLP systems. Then, in August 2022, they announced the release of BlenderBot 3, a chatbot designed to improve conversational skills and safety. In November 2022, Meta developed a large language model\n",
            "are moving us towards more holistic AI systems. These systems can potentially understand our world in a more comprehensive manner, bridging the gap between different forms of data and providing richer, more integrated solutions. As we stand on the cusp of this new era, it's exciting to envision the myriad of applications and innovations that Multimodal models will bring to the table. The future of AI looks more integrated and versatile than ever before.  From Connections to Vector DB The AI landscape is\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(docs)):\n",
        "    print(docs[i].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metadata_filter = {\n",
        "    \"result\": \"llama\"  # ChromaDB will perform a substring search\n",
        "}"
      ],
      "metadata": {
        "id": "H957cg-eXygP"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC03qAdpD74j",
        "outputId": "230c73e7-3cc5-4d72-d0ae-35602ae25158"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7f36cd7c5a50>, search_type='mmr', search_kwargs={'k': 3, 'fetch_k': 5, 'filter': {'result': 'llama'}})"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "retriever = vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3, \"fetch_k\":5, \"filter\": metadata_filter})\n",
        "retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clCH8Yl6EBrD",
        "outputId": "6c7ae78d-4561-44cc-aac2-c6b8f05dabe7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFaceEndpoint(repo_id='chsubhasis/ai-tutor-towardsai', huggingfacehub_api_token='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', top_k=30, temperature=0.1, repetition_penalty=1.03, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='chsubhasis/ai-tutor-towardsai', client=<InferenceClient(model='chsubhasis/ai-tutor-towardsai', timeout=120)>, async_client=<InferenceClient(model='chsubhasis/ai-tutor-towardsai', timeout=120)>, task='text-generation')"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "smCRo2M5MnzQ"
      },
      "outputs": [],
      "source": [
        "#Build a simple rag chain using retriever and llm\n",
        "\n",
        "from langchain.chains import RetrievalQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "jeW_Z_KZFpVZ"
      },
      "outputs": [],
      "source": [
        "def get_rag_response(query):\n",
        "  qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        "  )\n",
        "  result = qa_chain({\"query\": query})\n",
        "  return result[\"result\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fm3AkpNM9mU",
        "outputId": "e66d2406-3ff0-4be0-c0fe-24008258fe38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " AI refers specifically to artificial intelligence (AI). It's a combination of machine learning techniques and human-readable data gathered by humans on tasks like search or translation. This includes analyzing user feedback about what works best for them, as well as evaluating their responses based on relevant information from other users. Here's how it works:\n",
            "1. **Machine Learning** - Human input prompts each task with questions such as \"What are some useful skills do I need to learn?\", followed by suggestions tailored to those specific skill sets. These can be personalized through keyword searches, which might include language proficiency, attention span, etc., according to your needs. The goal is to find ways to enhance these abilities without compromising accuracy. For example—like using text chatbots instead of face recognition systems when searching for words—\"I want to understand more than just my personal preferences.\" Alternatively--such as creating customized guides designed for different roles within organizations, where possible—using real-time analytics to identify trends and insights into customer behavior.\n",
            "2. **Efficient Search Engines** - By leveraging natural language processing capabilities, automated queries become easier to interpret and query efficiently. They're often used alongside traditional databases or web services, allowing developers to tailor search results appropriately. Additionally, they offer improved performance compared to conventional methods due to fewer memory requirements and lower CPU usage. Traditional algorithms struggle because computational resources tend not to last long enough to handle large datasets, making efficient use of available space significantly less expensive.\n",
            "3. **Data Retrieval Techniques** - Data retrieval technologies enable rapid access to vast amounts of structured data, enabling researchers to analyze patterns, scenarios, and outcomes across multiple domains. In contrast, relational databases typically require extensive storage capacity and slow response times, requiring specialized hardware and software solutions. To address this limitation, several strategies have been developed to leverage existing infrastructure and processes to retrieve detailed historical records directly from external sources. One approach involves retrieving raw SQL commands rather then converting them manually to JSON format. Such tools allow applications to automate manual extraction while maintaining accurate state updates during deployment.\n",
            "4. **Retrieval Tools** - Enhanced predictive models facilitate effective decision-making and contextual understanding. Models capture key events related to a given event and provide recommendations regarding future actionable decisions. Users can also customize their own forecasts and behaviors accordingly. Moreover, retargeting offers enhanced efficiency if necessary, helping employers optimize resource utilization rates and aligning targeted activities with desired objectives.\n",
            "5. **Network Architecture & Platforms** - Integrated platforms help ensure robustness and scalability against distributed\n"
          ]
        }
      ],
      "source": [
        "print(get_rag_response(\"What is Artificial Intelligence?\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RAG Evaluation\n",
        "# Sample dataset of questions and expected answers\n",
        "dataset = [\n",
        "    {\"question\": \"Who is the CEO of Meta?\", \"expected_answer\": \"Mark Zuckerberg\"},\n",
        "    {\"question\": \"Who is the CEO of Apple?\", \"expected_answer\": \"Tiiiiiim Coooooook\"},\n",
        "]"
      ],
      "metadata": {
        "id": "qBUjKHDNljy-"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Function to evaluate Hit Rate and MRR\n",
        "def evaluate_rag(qa, dataset):\n",
        "    hits = 0\n",
        "    reciprocal_ranks = []\n",
        "\n",
        "    for entry in dataset:\n",
        "        question = entry[\"question\"]\n",
        "        expected_answer = entry[\"expected_answer\"]\n",
        "\n",
        "        # Get the answer from the RAG system\n",
        "        response = qa({\"query\": question})\n",
        "        answer = response[\"result\"]\n",
        "\n",
        "        # Check if the answer matches the expected answer\n",
        "        if expected_answer.lower() in answer.lower():\n",
        "            hits += 1\n",
        "            reciprocal_ranks.append(1)  # Hit at rank 1\n",
        "        else:\n",
        "            reciprocal_ranks.append(0)\n",
        "\n",
        "    # Calculate Hit Rate and MRR\n",
        "    hit_rate = hits / len(dataset)\n",
        "    mrr = np.mean(reciprocal_ranks)\n",
        "\n",
        "    return hit_rate, mrr"
      ],
      "metadata": {
        "id": "p0-aAr56mvSW"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        "  )"
      ],
      "metadata": {
        "id": "_2LtokgNm3nw"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the RAG system\n",
        "hit_rate, mrr = evaluate_rag(qa_chain, dataset)\n",
        "print(f\"Hit Rate: {hit_rate:.2f}, Mean Reciprocal Rank (MRR): {mrr:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmk5Y5tUmyx3",
        "outputId": "cba93dbb-ae9a-4614-bc5b-50e756bb26bb"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit Rate: 0.50, Mean Reciprocal Rank (MRR): 0.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1O81yziFenR"
      },
      "outputs": [],
      "source": [
        "!pip -qq install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTCyaHG6FfvC"
      },
      "outputs": [],
      "source": [
        "import gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-lAZXBRFjPS"
      },
      "outputs": [],
      "source": [
        "# Input from user\n",
        "in_question = gradio.Textbox(lines=10, placeholder=None, value=\"query\", label='Enter your query')\n",
        "\n",
        "# Output prediction\n",
        "out_response = gradio.Textbox(type=\"text\", label='RAG Response')\n",
        "\n",
        "\n",
        "# Gradio interface to generate UI\n",
        "iface = gradio.Interface(fn = get_rag_response,\n",
        "                         inputs = [in_question],\n",
        "                         outputs = [out_response],\n",
        "                         title = \"RAG Response\",\n",
        "                         description = \"Write the query and get the response from the RAG system\",\n",
        "                         allow_flagging = 'never')\n",
        "\n",
        "iface.launch(share = True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}