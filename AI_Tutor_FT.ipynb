{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBX22xX6_nUQ"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain-core\n",
        "!pip -q install langchain-community\n",
        "!pip -q install langchain_huggingface\n",
        "#!pip -q install langchain_chroma\n",
        "!pip -q install PyPDF2\n",
        "!pip -q install transformers\n",
        "!pip -q install datasets\n",
        "!pip -q install -U accelerate bitsandbytes peft trl\n",
        "!pip -q install jsonlines\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0uITRGf52hW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import PyPDF2\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, DataCollatorForLanguageModeling#\n",
        "#from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaGBbgN8biYV"
      },
      "outputs": [],
      "source": [
        "# Authentication for Huggingface API\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "hfapi_key = getpass(\"Enter you HuggingFace access token:\")\n",
        "os.environ[\"HF_TOKEN\"] = hfapi_key\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hfapi_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7991VSS4NVP"
      },
      "outputs": [],
      "source": [
        "# Function to read document pdf files\n",
        "def read_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "\n",
        "    # Open the PDF file\n",
        "    with open(pdf_path, \"rb\") as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        # Iterate over each page\n",
        "        for page_num in range(len(reader.pages)):\n",
        "            if page_num > 3:                         # extract text starting from page 5\n",
        "                page = reader.pages[page_num]\n",
        "                text += page.extract_text()\n",
        "\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5-mOW464UN4"
      },
      "outputs": [],
      "source": [
        "# Read files/documents\n",
        "\n",
        "pdf_path = 'AIML.pdf'\n",
        "text_file = read_pdf(pdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7K5USiC4YiV"
      },
      "outputs": [],
      "source": [
        "#print(text_file[:8000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYYNSELj4bre"
      },
      "outputs": [],
      "source": [
        "# Remove excess newline characters\n",
        "text_file = re.sub(r'\\n+', '\\n', text_file).strip()\n",
        "\n",
        "# Remove excess spaces\n",
        "text_file = re.sub(r' +', ' ', text_file).strip()\n",
        "\n",
        "# Remove unnecessary words (Header & Page number)\n",
        "text_file = re.sub(r' \\d+ International Gita Society', '', text_file)\n",
        "text_file = re.sub(r' Bhagavad -Gita \\d+', '', text_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrkGrPIR4iSo"
      },
      "outputs": [],
      "source": [
        "#print(text_file[:8000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fvonBZp4kIs"
      },
      "outputs": [],
      "source": [
        "#Keep 100 words per line inside text\n",
        "word_list = []\n",
        "new_text_file = ''\n",
        "\n",
        "for line in text_file.split('\\n'):\n",
        "    words = line.split()\n",
        "    for word in words:\n",
        "        word_list.append(word)\n",
        "        if len(word_list) == 100:\n",
        "            new_text_file += ' '.join(word_list) + '\\n'\n",
        "            word_list = []\n",
        "\n",
        "if word_list:\n",
        "    new_text_file += ' '.join(word_list) + '\\n'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEUWYWsd4omT"
      },
      "outputs": [],
      "source": [
        "#print(new_text_file[:8000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzcANUjT4p2u"
      },
      "outputs": [],
      "source": [
        "#len(new_text_file.split('\\n')[0].split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TArbhjKW4rpI"
      },
      "outputs": [],
      "source": [
        "# Split the text into training and validation sets\n",
        "\n",
        "train_fraction = 0.8\n",
        "split_index = int(train_fraction * len(new_text_file))\n",
        "\n",
        "train_text = new_text_file[:split_index]\n",
        "val_text = new_text_file[split_index:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSG-fgwV4s6m"
      },
      "outputs": [],
      "source": [
        "#len(train_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89UQMDTP4uD_"
      },
      "outputs": [],
      "source": [
        "# Save the training and validation data as text files\n",
        "\n",
        "with open(\"train.txt\", \"w\") as f:\n",
        "    f.write(train_text)\n",
        "\n",
        "with open(\"val.txt\", \"w\") as f:\n",
        "    f.write(val_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDbvRpv442mG"
      },
      "outputs": [],
      "source": [
        "# Set up the tokenizer\n",
        "checkpoint = \"openai-community/gpt2\"\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(checkpoint)    # also try gpt2, gpt2-large and gpt2-medium, also gpt2-xl\n",
        "\n",
        "# set pad_token_id to unk_token_id\n",
        "tokenizer.pad_token = tokenizer.unk_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oya_nYdE44Ao"
      },
      "outputs": [],
      "source": [
        "# Tokenize sample text using GP2Tokenizer\n",
        "sample_ids = tokenizer(\"Hello world\")\n",
        "sample_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHXqiO-C45T6"
      },
      "outputs": [],
      "source": [
        "# Generate tokens for sample text\n",
        "sample_tokens = tokenizer.convert_ids_to_tokens(sample_ids['input_ids'])\n",
        "sample_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ll-7KdPz46qq"
      },
      "outputs": [],
      "source": [
        "# Generate original text back\n",
        "tokenizer.convert_tokens_to_string(sample_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5v5K0G-zMuxq"
      },
      "outputs": [],
      "source": [
        "train_file_path = 'train.txt'\n",
        "val_file_path = 'val.txt'\n",
        "\n",
        "dataset = load_dataset(\"text\", data_files={\"train\": train_file_path,\n",
        "                                           \"validation\": val_file_path})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vfxNH3TM21U"
      },
      "outputs": [],
      "source": [
        "#dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epdjJGQkM_mE"
      },
      "outputs": [],
      "source": [
        "#dataset['train']['text'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBwvcf5UNBn8"
      },
      "outputs": [],
      "source": [
        "block_size = 256     # max tokens in an input sampleHuggingFace\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True, max_length=block_size, return_tensors='pt')\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Rc6S4S0NG7w"
      },
      "outputs": [],
      "source": [
        "#len(tokenized_datasets['train']['input_ids'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sca6dqCnNStu"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(tokenized_datasets['train']['input_ids'][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBpVFk8LNUof"
      },
      "outputs": [],
      "source": [
        "# Create a Data collator object\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux7NMk4ZNvsK"
      },
      "outputs": [],
      "source": [
        "# Set up the model\n",
        "model = GPT2LMHeadModel.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ey2N3lNcNZQ9"
      },
      "outputs": [],
      "source": [
        "# Set up the training arguments\n",
        "\n",
        "model_output_path = \"/content/tutor_model\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = model_output_path,\n",
        "    overwrite_output_dir = True,\n",
        "    per_device_train_batch_size = 4, # try with 2\n",
        "    per_device_eval_batch_size = 4,  #  try with 2\n",
        "    num_train_epochs = 100,\n",
        "    save_steps = 1_000,\n",
        "    save_total_limit = 2,\n",
        "    logging_dir = './logs',\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzLOw6KlNend"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    data_collator = data_collator,\n",
        "    train_dataset = tokenized_datasets[\"train\"],\n",
        "    eval_dataset = tokenized_datasets[\"validation\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yasRSlGhN8Vb"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqYjYzZnOTF-"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "saved_model_path = \"/content/finetuned_aitutor_model\"\n",
        "trainer.save_model(saved_model_path)\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained(saved_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xZtI9PyOabv"
      },
      "outputs": [],
      "source": [
        "def generate_response(model, tokenizer, prompt, max_length=200):\n",
        "\n",
        "    \"\"\"\n",
        "    Generate a response using the fine-tuned model\n",
        "\n",
        "    :param prompt: Input prompt\n",
        "    :param max_length: Maximum response length\n",
        "    :return: Generated text\n",
        "    \"\"\"\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        add_special_tokens=True\n",
        "    ).to(model.device)\n",
        "\n",
        "    # Generate response\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    # Decode and return response\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewk1lFNTUlmD"
      },
      "outputs": [],
      "source": [
        "# Load the fine-tuned model and tokenizer\n",
        "saved_model_path = \"/content/finetuned_aitutor_model\"\n",
        "my_model_finetuned = GPT2LMHeadModel.from_pretrained(saved_model_path)\n",
        "my_tokenizer_finetuned = GPT2Tokenizer.from_pretrained(saved_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cHLQIzXUnvv"
      },
      "outputs": [],
      "source": [
        "# Testing\n",
        "\n",
        "prompt = \"What is Artificial Intelligence?\"\n",
        "response = generate_response(my_model_finetuned, my_tokenizer_finetuned, prompt)\n",
        "print(\"Generated response:\")\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Push your fine-tuned model to HuggingFace Model Hub\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "Hdou_lohZloi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Push model\n",
        "my_repo = \"ai-tutor-towardsai-updated\"\n",
        "my_model_finetuned.push_to_hub(repo_id= my_repo, commit_message= \"Upload updated fine-tuned model\")"
      ],
      "metadata": {
        "id": "JsosNgL5Zmw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Push tokenizer\n",
        "my_tokenizer_finetuned.push_to_hub(repo_id= my_repo, commit_message= \"Upload updated tokenizer used\")"
      ],
      "metadata": {
        "id": "tvs6K-QpZpeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the model and tokenizer back from Hub and test it with user input prompts\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "\n",
        "my_checkpoint = \"chsubhasis/ai-tutor-towardsai-updated\"\n",
        "loaded_model = AutoModelWithLMHead.from_pretrained(my_checkpoint)\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(my_checkpoint)"
      ],
      "metadata": {
        "id": "p2EQABHLZtVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is Artifician Intelligence?\"           # Replace with your desired prompt\n",
        "response = generate_response(loaded_model, loaded_tokenizer, prompt)\n",
        "print(\"Generated response:\")\n",
        "response"
      ],
      "metadata": {
        "id": "9B-VbcnGZyqK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}